{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Intelligence Pipeline for Legal Analytics\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates an automated document ingestion and analysis pipeline for a legal firm. The pipeline:\n",
    "1. **Ingests** PDF documents (invoices, contracts, emails, meeting minutes)\n",
    "2. **Classifies** documents by type using Qwen LLM\n",
    "3. **Extracts** structured fields (client names, amounts, dates, parties)\n",
    "4. **Validates** and stores data for downstream analytics\n",
    "\n",
    "## Technical Stack\n",
    "- **PDF Extraction**: pdfplumber\n",
    "- **LLM**: Qwen 2.5 (7B) via Ollama\n",
    "- **Validation**: Pydantic\n",
    "- **Data Processing**: Pandas\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.ingestion import DocumentIngestor\n",
    "from src.classifier import DocumentClassifier\n",
    "from src.extractor import FieldExtractor\n",
    "from src.schemas import DocumentType, create_document\n",
    "from src.utils import (\n",
    "    save_to_json, \n",
    "    save_to_csv, \n",
    "    create_summary_report,\n",
    "    display_document_table,\n",
    "    validate_ollama_connection\n",
    ")\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Configure display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "\n",
    "print(\"âœ“ All modules imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Verify Ollama Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if Ollama is running and qwen2.5:7b is available\n",
    "is_connected = validate_ollama_connection(model_name=\"qwen2.5:7b\")\n",
    "\n",
    "if not is_connected:\n",
    "    print(\"\\nâš ï¸  Please ensure Ollama is running: ollama serve\")\n",
    "    print(\"âš ï¸  And qwen2.5:7b is installed: ollama pull qwen2.5:7b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Document Ingestion\n",
    "\n",
    "Extract text and metadata from PDF documents using pdfplumber."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the document ingestor\n",
    "ingestor = DocumentIngestor()\n",
    "\n",
    "# Ingest all PDFs from the input directory\n",
    "input_dir = \"../data/input\"\n",
    "documents = ingestor.batch_ingest(input_dir)\n",
    "\n",
    "print(f\"\\nðŸ“„ Successfully ingested {len(documents)} documents\\n\")\n",
    "\n",
    "# Display summary of ingested documents\n",
    "for i, doc in enumerate(documents, 1):\n",
    "    metadata = doc['metadata']\n",
    "    print(f\"{i}. {metadata['file_name']}\")\n",
    "    print(f\"   Pages: {metadata['num_pages']}\")\n",
    "    print(f\"   Size: {metadata['file_size'] / 1024:.1f} KB\")\n",
    "    print(f\"   Has Tables: {metadata['has_tables']}\")\n",
    "    print(f\"   Text Length: {len(doc['text'])} characters\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preview First Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if documents:\n",
    "    print(\"First 500 characters of the first document:\\n\")\n",
    "    print(documents[0]['text'][:500])\n",
    "    print(\"\\n...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Document Classification\n",
    "\n",
    "Use Qwen LLM to classify each document into one of four types:\n",
    "- Invoice\n",
    "- Contract\n",
    "- Email\n",
    "- Meeting Minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize classifier\n",
    "classifier = DocumentClassifier(model_name=\"qwen2.5:7b\")\n",
    "\n",
    "# Classify each document\n",
    "print(\"ðŸ¤– Classifying documents...\\n\")\n",
    "classifications = []\n",
    "\n",
    "for i, doc in enumerate(documents, 1):\n",
    "    print(f\"Classifying document {i}/{len(documents)}: {doc['metadata']['file_name']}\")\n",
    "    doc_type, confidence = classifier.classify(doc['text'])\n",
    "    \n",
    "    classifications.append({\n",
    "        'document': doc,\n",
    "        'type': doc_type,\n",
    "        'confidence': confidence\n",
    "    })\n",
    "    \n",
    "    print(f\"   â†’ Type: {doc_type.upper()} (confidence: {confidence:.2%})\\n\")\n",
    "\n",
    "print(\"âœ“ Classification complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary DataFrame\n",
    "classification_summary = pd.DataFrame([\n",
    "    {\n",
    "        'File': c['document']['metadata']['file_name'],\n",
    "        'Document Type': c['type'].upper(),\n",
    "        'Confidence': f\"{c['confidence']:.1%}\"\n",
    "    }\n",
    "    for c in classifications\n",
    "])\n",
    "\n",
    "display(classification_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Field Extraction\n",
    "\n",
    "Extract structured fields from each document based on its type:\n",
    "- **Invoice**: client name, amount, date, vendor, etc.\n",
    "- **Contract**: parties, value, dates, terms\n",
    "- **Email**: sender, recipients, subject, date\n",
    "- **Meeting Minutes**: attendees, agenda, decisions, action items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize extractor\n",
    "extractor = FieldExtractor(model_name=\"qwen2.5:7b\")\n",
    "\n",
    "# Extract fields from each document\n",
    "print(\"ðŸ” Extracting structured fields...\\n\")\n",
    "extracted_documents = []\n",
    "\n",
    "for i, item in enumerate(classifications, 1):\n",
    "    doc = item['document']\n",
    "    doc_type = item['type']\n",
    "    confidence = item['confidence']\n",
    "    \n",
    "    print(f\"Extracting from document {i}/{len(classifications)}: {doc['metadata']['file_name']}\")\n",
    "    \n",
    "    # Extract fields\n",
    "    extracted_fields = extractor.extract(doc['text'], doc_type)\n",
    "    \n",
    "    # Create document object using Pydantic schema\n",
    "    try:\n",
    "        document_obj = create_document(\n",
    "            doc_type=DocumentType(doc_type),\n",
    "            file_name=doc['metadata']['file_name'],\n",
    "            confidence_score=confidence,\n",
    "            **extracted_fields\n",
    "        )\n",
    "        extracted_documents.append(document_obj)\n",
    "        print(f\"   âœ“ Extracted {len(extracted_fields)} fields\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âš ï¸  Error creating document object: {str(e)}\\n\")\n",
    "\n",
    "print(f\"âœ“ Extraction complete: {len(extracted_documents)} documents processed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Extracted Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display extracted data for each document\n",
    "for i, doc in enumerate(extracted_documents, 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Document {i}: {doc.file_name}\")\n",
    "    print(f\"Type: {doc.document_type.upper()}\")\n",
    "    print(f\"Confidence: {doc.confidence_score:.1%}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Display in pretty JSON format (excluding raw_text)\n",
    "    doc_dict = doc.dict()\n",
    "    doc_dict.pop('raw_text', None)  # Remove raw text for cleaner display\n",
    "    print(json.dumps(doc_dict, indent=2, default=str))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Data Validation & Storage\n",
    "\n",
    "Validate extracted data using Pydantic schemas and save to:\n",
    "- Individual JSON files (for document retrieval)\n",
    "- Master CSV file (for analytics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to JSON (individual files)\n",
    "print(\"ðŸ’¾ Saving documents...\\n\")\n",
    "save_to_json(extracted_documents, output_dir=\"../data/output/json\")\n",
    "\n",
    "# Save to CSV (master file)\n",
    "df = save_to_csv(extracted_documents, output_file=\"../data/output/master_data.csv\")\n",
    "\n",
    "print(\"\\nâœ“ All documents saved successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preview Master CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows of the CSV\n",
    "print(\"Master Data CSV Preview:\\n\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Summary & Analytics\n",
    "\n",
    "Demonstrate downstream applications:\n",
    "- Summary statistics\n",
    "- Search capabilities\n",
    "- Aggregation examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary report\n",
    "summary = create_summary_report(extracted_documents)\n",
    "\n",
    "print(\"ðŸ“Š PROCESSING SUMMARY\\n\")\n",
    "print(f\"Total Documents Processed: {summary['total_documents']}\")\n",
    "print(f\"Average Confidence Score: {summary['average_confidence']:.1%}\")\n",
    "print(f\"\\nDocuments by Type:\")\n",
    "for doc_type, count in summary['documents_by_type'].items():\n",
    "    print(f\"  - {doc_type.upper()}: {count}\")\n",
    "\n",
    "if summary['total_invoice_amount'] > 0:\n",
    "    print(f\"\\nFinancial Summary:\")\n",
    "    print(f\"  Total Invoice Amount: ${summary['total_invoice_amount']:.2f}\")\n",
    "    print(f\"  Number of Invoices: {summary['num_invoices_with_amounts']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a clean summary table\n",
    "summary_table = display_document_table(extracted_documents)\n",
    "display(summary_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Downstream Use Cases\n",
    "\n",
    "### A. Search Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Find all invoices over a certain amount\n",
    "print(\"ðŸ” Search Example: Find invoices over $50\\n\")\n",
    "\n",
    "high_value_invoices = [\n",
    "    doc for doc in extracted_documents \n",
    "    if doc.document_type == 'invoice' and doc.total_amount and doc.total_amount > 50\n",
    "]\n",
    "\n",
    "if high_value_invoices:\n",
    "    for inv in high_value_invoices:\n",
    "        print(f\"- {inv.file_name}: ${inv.total_amount:.2f} from {inv.vendor_name}\")\n",
    "else:\n",
    "    print(\"No invoices found over $50\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Aggregation Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Total spending by vendor\n",
    "print(\"ðŸ“ˆ Aggregation Example: Total spending by vendor\\n\")\n",
    "\n",
    "vendor_totals = {}\n",
    "for doc in extracted_documents:\n",
    "    if doc.document_type == 'invoice' and doc.vendor_name and doc.total_amount:\n",
    "        vendor = doc.vendor_name\n",
    "        vendor_totals[vendor] = vendor_totals.get(vendor, 0) + doc.total_amount\n",
    "\n",
    "if vendor_totals:\n",
    "    for vendor, total in sorted(vendor_totals.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"- {vendor}: ${total:.2f}\")\n",
    "else:\n",
    "    print(\"No vendor data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Date-based Query Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Find all documents from a specific date range\n",
    "print(\"ðŸ“… Query Example: Documents from June 2025\\n\")\n",
    "\n",
    "june_docs = []\n",
    "for doc in extracted_documents:\n",
    "    date_field = None\n",
    "    if doc.document_type == 'invoice' and doc.invoice_date:\n",
    "        date_field = doc.invoice_date\n",
    "    elif doc.document_type == 'email' and doc.email_date:\n",
    "        date_field = doc.email_date\n",
    "    \n",
    "    if date_field and '2025-06' in str(date_field):\n",
    "        june_docs.append((doc.file_name, doc.document_type, date_field))\n",
    "\n",
    "if june_docs:\n",
    "    for filename, doc_type, date in june_docs:\n",
    "        print(f\"- {filename} ({doc_type}) - {date}\")\n",
    "else:\n",
    "    print(\"No documents found from June 2025\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Export for Reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for easy export to Excel/PowerBI\n",
    "print(\"ðŸ“¤ Creating export-ready DataFrame\\n\")\n",
    "\n",
    "# Invoice-specific export\n",
    "invoice_data = []\n",
    "for doc in extracted_documents:\n",
    "    if doc.document_type == 'invoice':\n",
    "        invoice_data.append({\n",
    "            'Invoice Number': doc.invoice_number,\n",
    "            'Date': doc.invoice_date,\n",
    "            'Client': doc.client_name,\n",
    "            'Vendor': doc.vendor_name,\n",
    "            'Amount': doc.total_amount,\n",
    "            'Currency': doc.currency,\n",
    "            'Confidence': f\"{doc.confidence_score:.1%}\"\n",
    "        })\n",
    "\n",
    "if invoice_data:\n",
    "    invoice_df = pd.DataFrame(invoice_data)\n",
    "    print(\"Invoice Summary for Reporting:\\n\")\n",
    "    display(invoice_df)\n",
    "    \n",
    "    # Save to CSV for Excel\n",
    "    invoice_df.to_csv('../data/output/invoice_report.csv', index=False)\n",
    "    print(\"\\nâœ“ Invoice report saved to data/output/invoice_report.csv\")\n",
    "else:\n",
    "    print(\"No invoice data to export\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Pipeline Capabilities Demonstrated:\n",
    "\n",
    "âœ… **Ingestion**: Successfully extracted text from PDF documents\n",
    "\n",
    "âœ… **Classification**: Accurately classified documents by type using LLM\n",
    "\n",
    "âœ… **Extraction**: Extracted all required fields:\n",
    "   - Client Name\n",
    "   - Invoice Amount\n",
    "   - Date(s)\n",
    "   - Involved Parties\n",
    "\n",
    "âœ… **Validation**: Used Pydantic schemas for data validation\n",
    "\n",
    "âœ… **Storage**: Saved in multiple formats (JSON, CSV) for different use cases\n",
    "\n",
    "âœ… **Analytics**: Demonstrated search, aggregation, and reporting capabilities\n",
    "\n",
    "### Next Steps for Production:\n",
    "- Add batch processing with parallel execution\n",
    "- Implement proper error handling and retry logic\n",
    "- Add vector database for semantic search\n",
    "- Create REST API endpoints\n",
    "- Add monitoring and logging\n",
    "- Implement data warehouse integration"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
